{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5a4bc8",
   "metadata": {},
   "source": [
    "#### Student Name:\n",
    "#### Student ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb2701",
   "metadata": {},
   "source": [
    "# Assignment 10\n",
    "\n",
    "### Transformer Models for Jazz Music Generation\n",
    "\n",
    "Instructions: \n",
    "\n",
    "* This notebook is an interactive assignment; please read and follow the instructions in each cell. \n",
    "\n",
    "* Cells that require your input (in the form of code or written response) will have 'Question #' above.\n",
    "\n",
    "* After completing the assignment, please submit this notebook and printout as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e377c",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This assignment guides you through building a transformer-based model for jazz music generation. You will download a dataset of MIDI files, preprocess it, build a small GPT model, train it and generate music. Some code is provided, other parts require your input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a238d16",
   "metadata": {},
   "source": [
    "### Downloading the Dataset\n",
    "\n",
    "Let's run the following cells to download the Doug McKenzie Jazz Piano MIDI dataset to the specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50584b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "DATA_URL = \"https://bushgrafts.com/midi/\"\n",
    "\n",
    "\n",
    "def download_dataset(dest_dir: str) -> None:\n",
    "    \"\"\"Download Doug McKenzie Jazz Piano MIDI dataset.\"\"\"\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    # Parse the index page with BeautifulSoup to reliably obtain all MIDI links\n",
    "    response = requests.get(DATA_URL)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    for tag in soup.find_all(\"a\", href=True):\n",
    "        href = tag[\"href\"]\n",
    "        if href.lower().endswith(\".mid\"):\n",
    "            url = href if href.startswith(\"http\") else DATA_URL + href\n",
    "            filename = os.path.basename(href)\n",
    "            out_path = os.path.join(dest_dir, filename)\n",
    "            if os.path.exists(out_path):\n",
    "                continue\n",
    "            resp = requests.get(url, stream=True)\n",
    "            resp.raise_for_status()\n",
    "            total = int(resp.headers.get(\"content-length\", 0))\n",
    "            with open(out_path, \"wb\") as f, tqdm(\n",
    "                total=total, unit=\"B\", unit_scale=True, desc=filename\n",
    "            ) as pbar:\n",
    "                for chunk in resp.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa77460",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset('jazz_data/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f71e40",
   "metadata": {},
   "source": [
    "##### Question 1 [8 points]\n",
    "\n",
    "The dataset contains several MIDI files. Complete the following function `midi_to_pianoroll` which converts a MIDI file into a piano roll representation with shape `(128, time_steps)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "DEFAULT_VELOCITY = 80\n",
    "TARGET_BPM = 120\n",
    "\n",
    "def midi_to_pianoroll(midi_path):\n",
    "    \"\"\"Convert a MIDI file to a piano roll.\n",
    "\n",
    "    Args:\n",
    "        midi_path (str): Path to a MIDI file.\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (128, T) with velocities.\n",
    "    \"\"\"\n",
    "    # Load the MIDI file with mido, clipping invalid bytes\n",
    "    mid = mido.MidiFile(midi_path, clip=True)\n",
    "    buf = BytesIO()\n",
    "    mid.save(file=buf)\n",
    "    buf.seek(0)\n",
    "    pm = pretty_midi.PrettyMIDI(buf)\n",
    "    # Technically, you could also use `pretty_midi.PrettyMIDI(midi_path)` directly,\n",
    "    # but using mido allows us to handle MIDI files with invalid bytes more gracefully.\n",
    "\n",
    "    # Here we will allocate a piano roll array.\n",
    "    # 128 = number of MIDI pitches (0-127)\n",
    "    # We will also quantize the data, such that each column in the piano roll corresponds to a 250ms time step.\n",
    "    # This is why we multiply the end time by 4 (since 1 second = 4 * 250ms).\n",
    "    roll = np.zeros((128, int(pm.get_end_time() * 4)), dtype=np.int16)\n",
    "    \n",
    "    # TODO: iterate over instruments and notes in pm to fill the roll\n",
    "    # for simplicity, you can ignore drum instruments and only consider melodic instruments\n",
    "    # (you can use .is_drum attribute of Instrument to check if an instrument is a drum)\n",
    "\n",
    "    # Hint: make sure you quantize the time steps correctly!\n",
    "    # Furthermore, if a note's velocity is not defined/set in the MIDI file, you can use the default velocity specified in the DEFAULT_VELOCITY constant.\n",
    "\n",
    "    # Your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb2c090",
   "metadata": {},
   "source": [
    "Test out your function by running the cell below after implementing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "roll = midi_to_pianoroll('jazz_data/raw/afine-1.mid')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_pianoroll(roll):\n",
    "    \"\"\"Plot a piano roll.\"\"\"\n",
    "    plt.imshow(roll, aspect='auto', origin='lower')\n",
    "    plt.xlabel('Time (1/4 notes)')\n",
    "    plt.ylabel('Pitch (MIDI note number)')\n",
    "    plt.colorbar(label='Velocity')\n",
    "    plt.title('Piano Roll')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Piano roll shape:\", roll.shape) # should be (128, T) where T is the number of time steps\n",
    "plot_pianoroll(roll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbdecc8",
   "metadata": {},
   "source": [
    "##### Question 2 [8 points]\n",
    "Using `midi_to_pianoroll`, write the function `preprocess_dataset` that iterates over all MIDI files in `raw_dir` and stores piano roll `.npy` arrays in `out_dir`. \n",
    "\n",
    "**Naming Convention:** If there is a MIDI file called `test.mid` in `raw_dir`, the output file should be `test.npy` in `out_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(raw_dir, out_dir):\n",
    "    \"\"\"Convert all MIDI files in raw_dir to piano rolls.\n",
    "\n",
    "    Args:\n",
    "        raw_dir (str): Directory containing MIDI files.\n",
    "        out_dir (str): Directory to save numpy arrays.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    # TODO: loop over files in raw_dir and save piano roll numpy arrays using midi_to_pianoroll\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1cb2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dataset('jazz_data/raw','jazz_data/processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c990eca1",
   "metadata": {},
   "source": [
    "##### Question 3 [8 points]\n",
    "\n",
    "Now that we have our piano rolls ready, we will tokenize them and build a vocabulary.\n",
    "\n",
    "A **vocabulary** is a dictionary that maps **each unique column** (consisting of all 128 pitches at a given time step) to an **integer** token. \n",
    "\n",
    "Each column is thus a tuple of 128 integers, where each integer represents the velocity of a pitch at that time step. For example, one such column could be as follows:\n",
    "\n",
    "```python\n",
    "(40, 0, 0, ..., 0)\n",
    "```\n",
    "\n",
    "Since the element at index 0 is 40, this means that the note with MIDI note number 0 has a velocity of 40 at that time step, while all other notes have a velocity of 0. \n",
    "\n",
    "So, your task is to:\n",
    "\n",
    "* Iterate over all `.npy` files obtained above.\n",
    "* Count the occurrences of each unique column (tuple of 128 integers).\n",
    "* Assign a unique integer token to each unique column in the following way: the column with the highest count gets token 2, the second highest gets token 3, the third highest gets token 4, and so on.\n",
    "* Token 0 is reserved for `<unk>` (unknown) columns. This is a special token for columns that do not match any of the known columns in the dataset.\n",
    "* Token 1 is reserved for `<pad>` (padding) columns. This is a special token for padding columns that are used to ensure that all sequences in a batch have the same length (we will use this later in training).\n",
    "\n",
    "In the end, the resulting dictionary might look something like this:\n",
    "\n",
    "```python\n",
    "vocab = {\n",
    "    (40,0,0,...,0): m,\n",
    "    ...,\n",
    "    (0,0,0,...,0): n,\n",
    "    '<unk>': 0,\n",
    "    '<pad>': 1\n",
    "}\n",
    "```\n",
    "\n",
    "where `m` and `n` are the tokens assigned to the respective columns and `m, n` are integers that are greater than or equal to 2.\n",
    "\n",
    "Save this dictionary with the `pickle.dump` method and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e01f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def build_vocab(data_dir, out_file=\"vocab.pkl\"):\n",
    "    \"\"\"Create a vocabulary dictionary from piano rolls.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory with piano roll `.npy` files.\n",
    "        out_file (str): Path to save the pickled vocabulary.\n",
    "    Returns:\n",
    "        dict: Mapping from column tuples to integer tokens.\n",
    "    \"\"\"\n",
    "    # TODO: populate counter with piano roll columns\n",
    "    # TODO: Save the resulting dictionary using pickle.dump, and return the vocabulary dictionary\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42468ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab('jazz_data/processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf19cf3",
   "metadata": {},
   "source": [
    "##### Question 4 [4 points]\n",
    "\n",
    "Fill in `tokenize_file` which converts a single piano roll `.npy` file to a sequence of integer tokens using a given vocabulary dictionary. This will involve iterating over each column in the piano roll, converting it to a tuple, and looking up the corresponding token in the vocabulary. Furthermore, if a column is not found in the vocabulary, it should be replaced with the `<unk>` token (0). This is important for generalizing the model to unseen data that may contain columns not present in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370b0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(fname, vocab):\n",
    "    \"\"\"Convert a piano roll array to a list of token ids.\n",
    "\n",
    "    Args:\n",
    "        fname (str): Path to `.npy` array.\n",
    "        vocab (dict): Mapping from column tuples to ints.\n",
    "    Returns:\n",
    "        np.ndarray: Sequence of integers (1D array).\n",
    "    \"\"\"\n",
    "    # TODO: load array and map each column to a token index\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab9e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_file('jazz_data/processed/align-1.npy', vocab)\n",
    "print(f\"Tokenized sequence length: {len(tokens)}\")\n",
    "print(f\"First 10 tokens: {tokens[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24166ad4",
   "metadata": {},
   "source": [
    "##### Question 5 [4 points]\n",
    "Write `tokenize_dataset` to convert every piano roll array in `src_dir` into a sequence of integers.\n",
    "Use the vocabulary built earlier. Columns not found in the vocabulary should map to the `<unk>` token (0).\n",
    "Pad sequences shorter than the desired length using the `<pad>` token (1) when needed. Save each token sequence as a `.npy` file in `out_dir`.\n",
    "\n",
    "**Naming Convention**: When saving the tokenized files, use the same base filename as the original `.npy` file, but with a `.npy` extension. For example, if the original file in `src_dir` is `align-1.npy`, the tokenized file written to `out_dir` should be `align-1.npy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e931a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(src_dir, vocab_file, out_dir):\n",
    "    \"\"\"Tokenize all piano rolls in a directory.\n",
    "\n",
    "    Args:\n",
    "        src_dir (str): Directory with piano roll arrays.\n",
    "        vocab_file (str): Path to saved vocabulary.\n",
    "        out_dir (str): Where to save token arrays.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    # TODO: load vocab and iterate over files saving token arrays\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d162145",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_dataset('jazz_data/processed', 'vocab.pkl', 'jazz_data/tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437134e7",
   "metadata": {},
   "source": [
    "##### Question 6 [6 points]\n",
    "\n",
    "Now you will implement a simple Layer Normalization module, which is an essential building block in transformer models.  \n",
    "Write this **from scratch** using TensorFlow tensor operations.\n",
    "\n",
    "Do **not** use `tf.keras.layers.LayerNormalization`. The point is for you to understand each part of the computation.\n",
    "\n",
    "Layer Normalization adjusts each feature vector so that its elements have mean zero and variance one, then applies a learned scale and shift. This helps stabilize training and gives the model flexibility to adapt the normalized outputs.\n",
    "\n",
    "Suppose your input `x` has shape `(B, T, C)`:\n",
    "- `B` is the batch size.\n",
    "- `T` is the sequence length.\n",
    "- `C` is the number of features.\n",
    "\n",
    "For each feature vector of length `C`:\n",
    "1. Compute the mean $\\mu$ along the feature dimension. This produces a tensor of shape `(B, T, 1)`.\n",
    "2. Compute the variance $\\sigma^2$ along the feature dimension. This also has shape `(B, T, 1)`.\n",
    "3. Normalize by subtracting the mean and dividing by the standard deviation, adding a small $\\epsilon$ to avoid division by zero.\n",
    "4. Apply a learned scale and shift:\n",
    "   - `weight` ($\\gamma$): shape `(C,)`, initialized to ones.\n",
    "   - `bias` ($\\beta$): shape `(C,)`, initialized to zeros.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "\\text{LN}(x)_i = \\gamma_i \\times \\Bigg( \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\Bigg) + \\beta_i.\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- For each `(B, T)` position, the mean and variance are scalar values, not full matrices. \n",
    "- The subtraction and division happen element-wise along the feature dimension.\n",
    "- The learned scale and shift also apply element-wise to each feature.\n",
    "\n",
    "So you are not dividing or multiplying full tensors of the same shape. You normalize each feature vector using its own scalar statistics, then scale and shift each feature independently.\n",
    "\n",
    "Use `unbiased=False` when computing the variance.  \n",
    "Your final output should have the same shape `(B, T, C)` as the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64248170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class LayerNorm(tf.keras.layers.Layer):\n",
    "    \"\"\"Layer normalization over the last dimension.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Feature dimension.\n",
    "        eps (float): Stability epsilon.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # TODO: create learnable gamma and beta parameters\n",
    "        pass\n",
    "    def call(self, x):\n",
    "        # TODO: normalize over the last dimension\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b40f774",
   "metadata": {},
   "source": [
    "##### Question 7 [8 points] \n",
    "\n",
    "In this part, you will write the multi-head self-attention computation from scratch using TensorFlow tensor operations. Do **not** use any built-in multi-head attention layers.\n",
    "\n",
    "Let the input be a batch of sequences $ X \\in \\mathbb{R}^{B \\times T \\times d} $, where $ B $ is the batch size, $ T $ is the sequence length, and $ d $ is the embedding dimension. In multi-head attention, we first project the input linearly to obtain the queries $ Q $, keys $ K $, and values $ V $. These are computed by applying a shared linear transformation:\n",
    "$$\n",
    "[Q, K, V] = X W, \\quad \\text{where } W = [W_q, W_k, W_v] \\in \\mathbb{R}^{d \\times 3d}\n",
    "$$\n",
    "Each of the resulting tensors $ Q, K, V \\in \\mathbb{R}^{B \\times T \\times d} $ is then split into $ h $ separate heads. Each head operates on a reduced dimension $ d_h = d / h $, reshaping the tensors into\n",
    "$$\n",
    "Q, K, V \\in \\mathbb{R}^{B \\times h \\times T \\times d_h}.\n",
    "$$\n",
    "\n",
    "Within each head, the attention scores are computed by scaled dot-product:\n",
    "$$\n",
    "A = \\frac{Q K^\\top}{\\sqrt{d_h}} \\in \\mathbb{R}^{B \\times h \\times T \\times T}.\n",
    "$$\n",
    "Then, we apply a lower-triangular mask $ M \\in \\mathbb{R}^{T \\times T} $, where\n",
    "$$\n",
    "M_{ij} =\n",
    "\\begin{cases}\n",
    "0 & \\text{if } j \\le i, \\\\\n",
    "-\\infty & \\text{if } j > i.\n",
    "\\end{cases}\n",
    "$$\n",
    "This mask is added to the attention scores:\n",
    "$$\n",
    "A' = A + M,\n",
    "$$\n",
    "so that positions can only attend to themselves and prior tokens in the sequence.\n",
    "\n",
    "We apply the softmax function over the last dimension of $ A' $ to obtain normalized attention weights:\n",
    "$$\n",
    "\\alpha = \\text{softmax}(A') \\in \\mathbb{R}^{B \\times h \\times T \\times T}.\n",
    "$$\n",
    "These weights are used to form a weighted sum of the values:\n",
    "$$\n",
    "Z = \\alpha V \\in \\mathbb{R}^{B \\times h \\times T \\times d_h}.\n",
    "$$\n",
    "\n",
    "Finally, the outputs from all heads are concatenated and projected back to the original dimension $ d $ using a linear transformation:\n",
    "$$\n",
    "\\text{Output} = \\text{ConcatHeads}(Z)\\, W_o + b, \\quad W_o \\in \\mathbb{R}^{d \\times d}.\n",
    "$$\n",
    "\n",
    "This completes the masked multi-head attention computation. Your implementation should follow this structure exactly: compute $ Q, K, V $, reshape for multiple heads, apply scaled dot-product attention with a causal mask, compute the weighted sum, concatenate the heads, and project back to the original feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, heads):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        # TODO: define qkv projection and output projection layers\n",
    "        pass\n",
    "    def call(self, x):\n",
    "        B = tf.shape(x)[0]\n",
    "        T = tf.shape(x)[1]\n",
    "        C = x.shape[-1]\n",
    "        # TODO: compute query, key, value tensors\n",
    "        # TODO: apply causal mask before softmax\n",
    "        # TODO: return attention output\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1028a4",
   "metadata": {},
   "source": [
    "##### Question 8 [6 points]\n",
    "Complete the `TransformerBlock` module using LayerNorm, SelfAttention and a feed-forward network. The FeedForward network is already given to you below, and is a basic two-layer MLP with GELU (Gaussian Error Linear Unit) activation.\n",
    "\n",
    "To understand the connections between these components, read the paper [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) from OpenAI.\n",
    "\n",
    "The input `x` and output of the block have shape `(B, T, C)`. Remember to create two LayerNorm layers, a SelfAttention module, a FeedForward network and dropout layers, then apply them with residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, hidden):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(hidden),\n",
    "            tf.keras.layers.Activation(tf.nn.gelu),\n",
    "            tf.keras.layers.Dense(dim)\n",
    "        ])\n",
    "    def call(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, heads, hidden, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # TODO: define LayerNorm, SelfAttention, FeedForward and dropout\n",
    "        pass\n",
    "    def call(self, x, training=False):\n",
    "        # TODO: apply attention and feed-forward with residual connections\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532d34c",
   "metadata": {},
   "source": [
    "##### Question 9 [10 points]\n",
    "\n",
    "Finish implementing the `GPT` model class so that it correctly follows the transformer architecture for autoregressive language modeling.\n",
    "\n",
    "The input is a tensor of token indices $x \\in \\mathbb{N}^{B \\times T}$, where $B$ is the batch size and $T$ is the sequence length. The model first embeds tokens and positions. The token embedding is a learned matrix $E \\in \\mathbb{R}^{V \\times d}$, where $V$ is the vocabulary size and $d$ is the embedding dimension. The lookup $E[x]$ produces a tensor in $\\mathbb{R}^{B \\times T \\times d}$. \n",
    "\n",
    "A positional embedding $P \\in \\mathbb{R}^{1 \\times L \\times d}$ provides position-dependent information, where $L$ is the maximum sequence length. For a given input length $T$, the slice $P[:, :T]$ has shape $(1, T, d)$ and broadcasts over the batch. The input to the transformer blocks is the sum of token and positional embeddings:\n",
    "$$\n",
    "X = E[x] + P[:, :T].\n",
    "$$\n",
    "\n",
    "**Important Note**: If you read the original [Attention Is All You Need](https://arxiv.org/pdf/1706.03762) paper, you might notice that the authors use something called \"positional encoding\" instead of positional embeddings. A positional encoding is typically a fixed function (like sine and cosine functions) that generates position-dependent vectors. However, in this assignment, we are using learned positional embeddings, which are more flexible and can adapt to the data during training. You can initialize the positional embedding matrix $P$ randomly.\n",
    "\n",
    "The model applies a stack of $N$ identical transformer blocks in sequence:\n",
    "$$\n",
    "H_0 = X,\\quad H_{i} = \\text{Block}_i(H_{i-1}),\\quad i = 1,\\dots,N.\n",
    "$$\n",
    "Each block preserves the shape, so $H_i \\in \\mathbb{R}^{B \\times T \\times d}$.\n",
    "\n",
    "After the last block, the output is normalized:\n",
    "$$\n",
    "H = \\text{LayerNorm}(H_N).\n",
    "$$\n",
    "\n",
    "Finally, the model projects this output to the vocabulary logits using a learned weight matrix $W_o \\in \\mathbb{R}^{d \\times V}$ and bias $b \\in \\mathbb{R}^{V}$:\n",
    "$$\n",
    "\\text{logits} = HW_o + b.\n",
    "$$\n",
    "\n",
    "The result $\\text{logits} \\in \\mathbb{R}^{B \\times T \\times V}$ gives the unnormalized scores for each token in the vocabulary at every position in the sequence. Make sure your implementation matches this sequence: embedding lookup, positional addition, stacked blocks, layer normalization, and final linear projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0ed286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, seq_len, dim=512, depth=8, heads=8, hidden=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # TODO: initialize token embedding, positional embedding, blocks, layer norm and output head\n",
    "        pass\n",
    "    def call(self, x, training=False):\n",
    "        T = tf.shape(x)[1]\n",
    "        # TODO: implement forward pass following the architecture description\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b3b45",
   "metadata": {},
   "source": [
    "##### Question 10 [10 points]\n",
    "Complete the training loop below. For each mini-batch $(x,y)$ compute logits $L = \\text{GPT}(x)$ and minimize\n",
    "the cross-entropy loss\n",
    "$$\\mathcal{L} = -\\frac{1}{N}\\sum_i y_i \\log \\text{softmax}(L_i).$$\n",
    "Save model weights to `model_epoch{epoch}.pt` after every epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86def13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class TokenDataset(tf.data.Dataset):\n",
    "    def __new__(cls, root, seq_len=128):\n",
    "        files = [os.path.join(root, f) for f in os.listdir(root) if f.endswith('.npy')]\n",
    "        def gen():\n",
    "            # TODO: yield (x, y) pairs of length seq_len from token arrays\n",
    "            pass\n",
    "        return tf.data.Dataset.from_generator(gen, output_signature=(tf.TensorSpec((seq_len,), tf.int32), tf.TensorSpec((seq_len,), tf.int32)))\n",
    "\n",
    "def train(data_dir, vocab_file, seq_len=128, batch_size=16, epochs=10, lr=3e-4, device='cuda'):\n",
    "    \"\"\"Train the GPT model on tokenized data.\"\"\"\n",
    "    # TODO: implement training loop using Keras optimizers and gradients\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491f75de",
   "metadata": {},
   "source": [
    "Feel free to adjust the hyperparameters such as learning rate, batch size, and number of epochs to achieve better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train('jazz_data/tokens','vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa7a92",
   "metadata": {},
   "source": [
    "##### Question 11 [8 points]\n",
    "Implement the `generate` function below which autoregressively samples from the model.\n",
    "Use the trained weights and convert the predicted tokens back to a piano roll.\n",
    "\n",
    "Your code should:\n",
    "1. Load the trained model checkpoint from the file path `model_path`.\n",
    "2. Feed the provided `seed` tokens to the model and iteratively sample one new token at a time.\n",
    "3. Append each sampled token to the context and continue for `steps` iterations.\n",
    "4. Convert the resulting token sequence back to a piano roll using `tokens_to_pianoroll` and `pianoroll_to_pretty_midi`.\n",
    "\n",
    "Note that the model outputs logits for the next token at each step, which you can sample from using `tf.random.categorical`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65afc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "SECONDS_PER_STEP = 0.25\n",
    "\n",
    "def tokens_to_pianoroll(tokens):\n",
    "    cols = []\n",
    "    for t in tokens:\n",
    "        if isinstance(t, str):\n",
    "            cols.append(np.zeros(128, dtype=np.int16))\n",
    "        else:\n",
    "            cols.append(np.array(t, dtype=np.int16))\n",
    "    if not cols:\n",
    "        return np.zeros((128,0), dtype=np.int16)\n",
    "    return np.stack(cols, axis=1)\n",
    "\n",
    "def pianoroll_to_pretty_midi(roll):\n",
    "    pm = pretty_midi.PrettyMIDI(initial_tempo=120)\n",
    "    instrument = pretty_midi.Instrument(program=0)\n",
    "    num_steps = roll.shape[1]\n",
    "    for pitch in range(roll.shape[0]):\n",
    "        velocity_series = roll[pitch]\n",
    "        prev_vel = 0\n",
    "        start = 0\n",
    "        for t in range(num_steps):\n",
    "            vel = int(velocity_series[t])\n",
    "            if vel > 0 and prev_vel == 0:\n",
    "                start = t\n",
    "                prev_vel = vel\n",
    "            elif vel != prev_vel:\n",
    "                if prev_vel > 0:\n",
    "                    note = pretty_midi.Note(velocity=int(prev_vel), pitch=pitch, start=start*SECONDS_PER_STEP, end=t*SECONDS_PER_STEP)\n",
    "                    instrument.notes.append(note)\n",
    "                if vel > 0:\n",
    "                    start = t\n",
    "                prev_vel = vel\n",
    "        if prev_vel > 0:\n",
    "            note = pretty_midi.Note(velocity=int(prev_vel), pitch=pitch, start=start*SECONDS_PER_STEP, end=num_steps*SECONDS_PER_STEP)\n",
    "            instrument.notes.append(note)\n",
    "    pm.instruments.append(instrument)\n",
    "    return pm\n",
    "\n",
    "def generate(model_path, vocab_file, seed, steps=100, seq_len=128, device='cuda'):\n",
    "    # TODO: load vocab and model, then autoregressively generate new tokens\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28255664",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = generate('model_epoch10.pt','vocab.pkl',[1],steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_audio(pm: pretty_midi.PrettyMIDI, path: str, sample_rate: int = 44100) -> None:\n",
    "    \"\"\"Save a :class:`pretty_midi.PrettyMIDI` object as a WAV file.\"\"\"\n",
    "    audio = pm.fluidsynth(fs=sample_rate)\n",
    "    wavfile.write(path, sample_rate, audio.astype(np.float32))\n",
    "\n",
    "pm.write('generated.mid') # Save the MIDI file\n",
    "save_audio(pm, 'generated.wav') # Save the audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26846f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio('generated.wav')  # Play the generated audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a66c70",
   "metadata": {},
   "source": [
    "##### Question 12 [4 points]\n",
    "After you have trained your model and generated some samples, briefly describe the quality of the generated music. What worked well and what could be improved? You can change the seed tokens to generate different samples and see how the model performs with different starting points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42d9829",
   "metadata": {},
   "source": [
    "`Your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f876f75",
   "metadata": {},
   "source": [
    "##### Question 13 [4 points]\n",
    "Explain why masking in self-attention is necessary for autoregressive generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386c89e",
   "metadata": {},
   "source": [
    "`Your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e4ce10",
   "metadata": {},
   "source": [
    "##### Question 14 [4 points]\n",
    "Why do we tokenize the piano roll instead of feeding the raw continuous values directly into the transformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225c9786",
   "metadata": {},
   "source": [
    "`Your answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61817712",
   "metadata": {},
   "source": [
    "##### Question 15 [8 points]\n",
    "Suggest two hyperparameters you might tune to improve generation quality and briefly justify your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02442882",
   "metadata": {},
   "source": [
    "`Your answer here`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}